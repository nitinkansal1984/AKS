##Create resource group
az group create -l eastus -n MyResourceGroup

##Launch 1 node AKS Cluster
az aks create --resource-group myResourceGroup --name myAKSCluster --node-count 1 

## Let's now get the credentials of the cluster and connect to it.
az aks get-credentials --resource-group MyResourceGroup --name MyAKSCluster --overwrite

## Lets see the number of nodes part of cluster
kubectl get nodes

## Let's verify all system workloads running inside namespace kube-system on to system node pool.
kubectl get pods -n kube-system -o wide

## Add second system pool
You can add one or more system node pools to existing AKS clusters. 
It's recommended to schedule your application pods on user node pools, and dedicate system node pools to only critical system pods. 
This prevents rogue application pods from accidentally killing system pods. 
Enforce this behavior with the CriticalAddonsOnly=true:NoSchedule taint for your system node pools.

az aks nodepool add -g MyResourceGroup \
                    -n sysnodepool \
                    --cluster-name MyAKSCluster \
                    --os-sku Ubuntu \
                    --node-count 1 \
                    --mode System \
                    --node-taints CriticalAddonsOnly=true:NoSchedule

## Lets add a usernodepool
az aks nodepool add -g MyResourceGroup -n usernodepool --cluster-name MyAKSCluster --os-sku Ubuntu --node-count 1

## Command to see name of all node pool and number of nodes in node pools
az aks show --resource-group myResourceGroup --name myAKSCluster --query agentPoolProfiles

## Go to AKS blade and notice the system pool named nodepool and number of nodes. Also notice the option to scale the number of nodes from there directly.

## Let's now scale the cluster to 2 worker nodes
az aks scale --resource-group myResourceGroup --name myAKSCluster --node-count 2 --nodepool-name usernodepool

## Scale it back to 1
az aks scale --resource-group myResourceGroup --name myAKSCluster --node-count 1 --nodepool-name usernodepool

## Lets verify the number of nodes now
kubectl get nodes

#NOTE: You can not scale down system nodepool to 0 nodes, it always need minimum 1 running node. But you can scale down user nodepool to 0.
#Let's verify this:
az aks nodepool scale --name nodepool1 --cluster-name myAKSCluster --resource-group myResourceGroup  --node-count 0

## Now Lets verify scaling down usernodepool to ZERO
az aks nodepool scale --name usernodepool --cluster-name myAKSCluster --resource-group myResourceGroup  --node-count 0


## Let's go to AKS blade nodepool and delete default system nodepool nodepool1 and notice all system workloads transitioned to another systemnodepool.
## Let's try deleting the last systempool from portal. It will not allow.
## Let's convert usernodepool in to systempool.
az aks nodepool update -g myResourceGroup --cluster-name myAKSCluster -n usernodepool --mode system
az aks nodepool update -g myResourceGroup --cluster-name myAKSCluster -n usernodepool --mode user

## If we try converting existing system node pool to user node pool, it will fail, because we have to have at least one system pool running all time.

az aks nodepool update -g myResourceGroup --cluster-name myAKSCluster -n sysnodepool --mode user


In this demo, we have seen manually scaling up/down of system nodepool. In next lecture we will learn automatic scaling. Thanks !

